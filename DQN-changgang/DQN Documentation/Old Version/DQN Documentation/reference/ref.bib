@article{1,
  title={UAV-assisted heterogeneous networks for capacity enhancement},
  author={Sharma, Vishal and Bennis, Mehdi and Kumar, Rajesh},
  journal={IEEE Communications Letters},
  volume={20},
  number={6},
  pages={1207--1210},
  year={2016},
  publisher={IEEE}
}
@inproceedings{2,
  title={UAV relay network to support WSN connectivity},
  author={De Freitas, Edison Pignaton and Heimfarth, Tales and Netto, Ivayr Farah and Lino, Carlos Eduardo and Pereira, Carlos Eduardo and Ferreira, Armando Morado and Wagner, Fl{\'a}vio Rech and Larsson, Tony},
  booktitle={International Congress on Ultra Modern Telecommunications and Control Systems},
  pages={309--314},
  year={2010},
  organization={IEEE}
}
@inproceedings{3,
  title={UAV assisted heterogeneous networks for public safety communications},
  author={Merwaday, Arvind and Guvenc, Ismail},
  booktitle={2015 IEEE wireless communications and networking conference workshops (WCNCW)},
  pages={329--334},
  year={2015},
  organization={IEEE}
}
@article{4,
  title={Wireless communications with unmanned aerial vehicles: Opportunities and challenges},
  author={Zeng, Yong and Zhang, Rui and Lim, Teng Joon},
  journal={IEEE Communications Magazine},
  volume={54},
  number={5},
  pages={36--42},
  year={2016},
  publisher={IEEE}
}
@article{5,
  title={Help from the sky: Leveraging UAVs for disaster management},
  author={Erdelj, Milan and Natalizio, Enrico and Chowdhury, Kaushik R and Akyildiz, Ian F},
  journal={IEEE Pervasive Computing},
  volume={16},
  number={1},
  pages={24--32},
  year={2017},
  publisher={IEEE}
}
@article{6,
  title={Base station location and channel allocation in a cellular network with emergency coverage requirements},
  author={Akella, Mohan R and Batta, Rajan and Delmelle, Eric M and Rogerson, Peter A and Blatt, Alan and Wilson, Glenn},
  journal={European Journal of Operational Research},
  volume={164},
  number={2},
  pages={301--323},
  year={2005},
  publisher={Elsevier}
}
@article{7,
  title={Placement optimization of UAV-mounted mobile base stations},
  author={Lyu, Jiangbin and Zeng, Yong and Zhang, Rui and Lim, Teng Joon},
  journal={IEEE Communications Letters},
  volume={21},
  number={3},
  pages={604--607},
  year={2016},
  publisher={IEEE}
}
@article{8,
  title={3-D placement of an unmanned aerial vehicle base station (UAV-BS) for energy-efficient maximal coverage},
  author={Alzenad, Mohamed and El-Keyi, Amr and Lagum, Faraj and Yanikomeroglu, Halim},
  journal={IEEE Wireless Communications Letters},
  volume={6},
  number={4},
  pages={434--437},
  year={2017},
  publisher={IEEE}
}
@article{9,
  title={3-D placement of an unmanned aerial vehicle base station for maximum coverage of users with different QoS requirements},
  author={Alzenad, Mohamed and El-Keyi, Amr and Yanikomeroglu, Halim},
  journal={IEEE Wireless Communications Letters},
  volume={7},
  number={1},
  pages={38--41},
  year={2017},
  publisher={IEEE}
}
@inproceedings{a,
  title={On the number and 3D placement of drone base stations in wireless cellular networks},
  author={Kalantari, Elham and Yanikomeroglu, Halim and Yongacoglu, Abbas},
  booktitle={2016 IEEE 84th Vehicular Technology Conference (VTC-Fall)},
  pages={1--6},
  year={2016},
  organization={IEEE}
}
@article{b,
  title={Distributed drone base station positioning for emergency cellular networks using reinforcement learning},
  author={Klaine, Paulo V and Nadas, Jo{\~a}o PB and Souza, Richard D and Imran, Muhammad A},
  journal={Cognitive computation},
  volume={10},
  number={5},
  pages={790--804},
  year={2018},
  publisher={Springer}
}
@article{c,
  title={Cooperative and distributed reinforcement learning of drones for field coverage},
  author={Pham, Huy Xuan and La, Hung Manh and Feil-Seifer, David and Nefian, Aria},
  journal={arXiv preprint arXiv:1803.07250},
  year={2018}
}
@article{d,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{e,
  title={Reinforcement learning: A tutorial survey and recent advances},
  author={Gosavi, Abhijit},
  journal={INFORMS Journal on Computing},
  volume={21},
  number={2},
  pages={178--192},
  year={2009},
  publisher={INFORMS}
}
@article{f,
  title={A modern approach to distributed artificial intelligence},
  author={Weiss, Gerhard},
  journal={IEEE transactions on systems man \& cybernetics-part c applications \& reviews},
  volume={22},
  number={2},
  year={1999}
}
@article{g,
  title={A comprehensive survey of multiagent reinforcement learning},
  author={Bu, Lucian and Babu, Robert and De Schutter, Bart and others},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={38},
  number={2},
  pages={156--172},
  year={2008},
  publisher={IEEE}
}


@article{j,
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  volume={4},
  pages={237--285},
  year={1996}
}
@article{k,
  title={Model-free control},
  author={Fliess, Michel and Join, C{\'e}dric},
  journal={International Journal of Control},
  volume={86},
  number={12},
  pages={2228--2252},
  year={2013},
  publisher={Taylor \& Francis}
}


@inproceedings{r,
  title={Towards Smart Cache Management for Ontology Based, History-Aware Stream Reasoning.},
  author={Yan, Rui and Praggastis, Brenda and Smith, William P and McGuinness, Deborah L},
  booktitle={LISC \& ISWC},
  pages={38--41},
  year={2015}
}

@inproceedings{s,
  title={History-aware self-scheduling},
  author={Kejariwal, Arun and Nicolau, Alexandru and Polychronopoulos, Constantine D},
  booktitle={2006 International Conference on Parallel Processing (ICPP'06)},
  pages={185--192},
  year={2006},
  organization={IEEE}
}

@inproceedings{t,
  title={History-aware autonomous exploration in confined environments using MAVs},
  author={Witting, Christian and Fehr, Marius and B{\"a}hnemann, Rik and Oleynikova, Helen and Siegwart, Roland},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={1--9},
  year={2018},
  organization={IEEE}
}


@inproceedings{u,
  title={Reflecting on the past and the present with temporal graph-based models},
  author={Garc{\'\i}a-Dom{\'\i}nguez, Antonio and Bencomo, Nelly and Garcia Paucar, Luis H},
  booktitle={CEUR Workshop Proceedings},
  volume={2245},
  pages={46--55},
  year={2018}
}


@phdthesis{00,
  title={Learning from delayed rewards},
  author={Watkins, Christopher John Cornish Hellaby},
  year={1989},
  school={King's College, Cambridge}
}

@article{01,
  title = {Q-learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  issn = {0885-6125},
  journal = {Machine Learning},
  number = {3-4},
  pages = {279--292},
  title = {{Q-learning}},
  volume = {8},
  year = {1992}
}
@inproceedings{02,
  title={Generalization in reinforcement learning: Successful examples using sparse coarse coding},
  author={Sutton, Richard S},
  booktitle={Advances in neural information processing systems},
  pages={1038--1044},
  year={1996}
}
@article{03,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{04,
abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
archivePrefix = {arXiv},
arxivId = {1710.02298},
author = {Hessel, Matteo and Modayil, Joseph and {Van Hasselt}, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
eprint = {1710.02298},
file = {:Users/changgang/Documents/Mendeley Desktop/Rainbow$\backslash$: Combining Improvements in Deep Reinforcement Learning.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
keywords = {Machine Learning Methods Track},
pages = {3215--3222},
title = {{Rainbow: Combining improvements in deep reinforcement learning}},
year = {2018}
}


@article{05,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
file = {:Users/changgang/Documents/Mendeley Desktop/Mnih et al. - Unknown - Playing Atari with Deep Reinforcement Learning.pdf:pdf},
pages = {1--9},
title = {{Playing Atari with Deep Reinforcement Learning}},
}


@article{06,
  title={SINR, RSRP, RSSI and RSRQ measurements in long term evolution networks},
  author={Afroz, Farhana and Subramanian, Ramprasad and Heidary, Roshanak and Sandrasegaran, Kumbesan and Ahmed, Solaiman},
  journal={International Journal of Wireless \& Mobile Networks},
  year={2015},
  publisher={Academy and Industry Research Collaboration Center (AIRCC)}
}
@article{07,
  title={RSRP and RSRQ measurement in LTE},
  author={La Rocca, Maurizio},
  journal={laroccasolutions Technology \& Services, Feb},
  volume={2},
  pages={9},
  year={2015}
}

@inproceedings{08,
  title={Friendship and mobility: user movement in location-based social networks},
  author={Cho, Eunjoon and Myers, Seth A and Leskovec, Jure},
  booktitle={Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={1082--1090},
  year={2011},
  organization={ACM}
}

@inproceedings{09,
  title={Exploring social-historical ties on location-based social networks},
  author={Gao, Huiji and Tang, Jiliang and Liu, Huan},
  booktitle={Sixth International AAAI Conference on Weblogs and Social Media},
  year={2012}
}

@book{0a,
  title={Estimating population density distribution from network-based mobile phone data},
  author={Ricciato, Fabio and Widhalm, Peter and Craglia, Massimo and Pantisano, Francesco},
  year={2015},
  publisher={Publications Office of the European Union}
}

@article{0b,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy-that is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actorcritic formulation, our method achieves state-ofthe-art performance on a range of continuous control benchmark tasks, outperforming prior onpolicy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
archivePrefix = {arXiv},
arxivId = {1801.01290},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
eprint = {1801.01290},
file = {:Users/changgang/Documents/Mendeley Desktop/Soft Actor-Critic$\backslash$: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
pages = {2976--2989},
title = {{Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor}},
volume = {5},
year = {2018}
}

@book{0c,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{0d,
  title={Dynamic Multiobjective Control for Continuous-Time Systems Using Reinforcement Learning},
  author={Lopez, Victor G and Lewis, Frank L},
  journal={IEEE Transactions on Automatic Control},
  volume={64},
  number={7},
  pages={2869--2874},
  year={2019},
  publisher={IEEE-Institute of Electrical and Electronics Engineers, Inc., New York, USA}
}

@article{0e,
  title={Integrating models of interval timing and reinforcement learning},
  author={Petter, Elijah A and Gershman, Samuel J and Meck, Warren H},
  journal={Trends in cognitive sciences},
  volume={22},
  number={10},
  pages={911--922},
  year={2018},
  publisher={Elsevier}
}

@article{0f,
author = {Murphy, Kevin},
year = {2007},
month = {11},
pages = {1-28},
title = {Conjugate Bayesian analysis of the Gaussian distribution}
}

@article{0g,
  title={A review of robust clustering methods},
  author={Garc{\'\i}a-Escudero, Luis Angel and Gordaliza, Alfonso and Matr{\'a}n, Carlos and Mayo-Iscar, Agust{\'\i}n},
  journal={Advances in Data Analysis and Classification},
  volume={4},
  number={2-3},
  pages={89--109},
  year={2010},
  publisher={Springer}
}

@article{0h,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
file = {:Users/changgang/Documents/Mendeley Desktop/1412.6572.pdf).pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--11},
title = {{Explaining and harnessing adversarial examples}},
year = {2015}
}


@article{0i,
abstract = {Recent studies have highlighted the vulnerability of deep neural networks (DNNs) to adversarial examples - a visually indistinguishable adversarial image can easily be crafted to cause a well-trained model to misclassify. Existing methods for crafting adversarial examples are based on L2 and L∞ distortion metrics. However, despite the fact that L1 distortion accounts for the total variation and encourages sparsity in the perturbation, little has been developed for crafting L1-based adversarial examples. In this paper, we formulate the process of attacking DNNs via adversarial examples as an elastic-net regularized optimization problem. Our elastic-net attacks to DNNs (EAD) feature L1-oriented adversarial examples and include the state-of-the-art L2 attack as a special case. Experimental results on MNIST, CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial examples with small L1 distortion and attains similar attack performance to the state-of-the-art methods in different attack scenarios. More importantly, EAD leads to improved attack transferability and complements adversarial training for DNNs, suggesting novel insights on leveraging L1 distortion in adversarial machine learning and security implications of DNNs.},
archivePrefix = {arXiv},
arxivId = {1709.04114},
author = {Chen, Pin Yu and Sharma, Yash and Zhang, Huan and Yi, Jinfeng and Hsieh, Cho Jui},
eprint = {1709.04114},
file = {:Users/changgang/Documents/Mendeley Desktop/16893-76424-1-PB.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
keywords = {Applications Track},
pages = {10--17},
title = {{EAD: Elastic-net attacks to deep neural networks via adversarial examples}},
year = {2018}
}
@article{0j,
abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95{\%} to 0.5{\%}.In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100{\%} probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
archivePrefix = {arXiv},
arxivId = {1608.04644},
author = {Carlini, Nicholas and Wagner, David},
doi = {10.1109/SP.2017.49},
eprint = {1608.04644},
file = {:Users/changgang/Documents/Mendeley Desktop/1608.04644.pdf:pdf},
isbn = {9781509055326},
issn = {10816011},
journal = {Proceedings - IEEE Symposium on Security and Privacy},
pages = {39--57},
title = {{Towards Evaluating the Robustness of Neural Networks}},
year = {2017}
}
@article{0k,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
file = {:Users/changgang/Documents/Mendeley Desktop/1412.6572.pdf).pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--11},
title = {{Explaining and harnessing adversarial examples}},
year = {2015}
}
